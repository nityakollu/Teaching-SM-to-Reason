# Essential Files Guide - Teaching Small Models to Reason

## âœ… ESSENTIAL SCRIPTS (Keep These)

### Core Pipeline Scripts
1. **`scripts/run_pipeline.py`** - Main pipeline orchestrator (runs all steps)
2. **`scripts/dataset_generation/generate_examples.py`** - Generate datasets with Ollama
3. **`scripts/cleaning/filter_bad.py`** - Filter valid vs broken SMT programs
4. **`scripts/cleaning/repair_smt.py`** - Deterministic template-based repair
5. **`scripts/verify/check_validity.py`** - Z3 validation of SMT programs
6. **`scripts/distillation/format_jsonl.py`** - Convert CSV to JSONL format
7. **`scripts/distillation/finetune.py`** - LoRA fine-tuning script
8. **`scripts/refine/refiner.py`** - Solver-in-the-loop refinement
9. **`scripts/eval/evaluate.py`** - Final evaluation comparing baseline/distilled/refined

### Supporting Files
- **`requirements.txt`** - Python dependencies
- **`README.md`** - Project documentation
- **`QUICK_START.md`** - Quick start guide
- **`SETUP.md`** - Setup instructions

## âŒ DUPLICATE/UNUSED SCRIPTS (Can Delete)

### Dataset Generation Duplicates
- âŒ `scripts/dataset_generation/generate.py` - **DUPLICATE** (use `generate_examples.py` instead)

### Cleaning Scripts - Unused
- âŒ `scripts/cleaning/analyze_scheduling.py` - Analysis tool, not part of pipeline
- âŒ `scripts/cleaning/check_scheduling.py` - Analysis tool, not part of pipeline
- âŒ `scripts/cleaning/check_scheduling_simple.py` - Analysis tool, not part of pipeline
- âŒ `scripts/cleaning/clean_examples.py` - **DUPLICATE/UNUSED** (use `filter_bad.py` + `repair_smt.py`)

### Distillation Duplicates
- âŒ `scripts/distillation/finetune_lora.py` - **DUPLICATE** (use `finetune.py` instead)
- âŒ `scripts/distillation/format_for_training.py` - **DUPLICATE** (use `format_jsonl.py` instead)

### Refine Duplicates
- âŒ `scripts/refine/iterative_refiner.py` - **DUPLICATE** (use `refiner.py` instead)

### Evaluation Duplicates
- âŒ `scripts/eval/evaluate_models.py` - **DUPLICATE** (use `evaluate.py` instead)

### Other
- âŒ `scripts/generate_paper.py` - Optional, can be kept if needed

## ğŸ“ ESSENTIAL DATA DIRECTORIES

### Input Data (Required)
```
data/raw/
  â”œâ”€â”€ scheduling.csv          # Required: Raw scheduling problems
  â”œâ”€â”€ puzzles.csv             # Required: Raw puzzle problems
  â”œâ”€â”€ relationships.csv       # Required: Raw relationship problems
  â”œâ”€â”€ resource_alloc.csv      # Required: Raw resource allocation problems
  â””â”€â”€ broken.csv              # Generated: Broken examples from filtering
```

### Intermediate Data (Generated)
```
data/cleaned/
  â”œâ”€â”€ valid.csv               # Generated: Valid examples after filtering
  â”œâ”€â”€ repaired.csv            # Generated: Repaired examples
  â””â”€â”€ training.jsonl          # Generated: Training data in JSONL format
```

### Evaluation Data (Generated)
```
data/eval/
  â”œâ”€â”€ validation_results.csv  # Generated: Z3 validation results
  â”œâ”€â”€ baseline_outputs.csv    # Generated: Baseline model outputs
  â”œâ”€â”€ distilled_outputs.csv   # Generated: Distilled model outputs
  â”œâ”€â”€ refined_outputs.csv     # Generated: Refined outputs
  â””â”€â”€ final_metrics.json      # Generated: Final evaluation metrics
```

## ğŸ—‘ï¸ DATA FILES TO CLEAN UP

### Duplicate/Old Data Files
- âŒ `data/cleaned/cleaned_dataset.csv` - **OLD/DEPRECATED** (use `valid.csv` + `repaired.csv` instead)
- âŒ `data/cleaned/jsonl/training_dataset.jsonl` - **DUPLICATE** (use `training.jsonl` instead)
- âŒ `data/eval/eval_set.csv` - **EMPTY/UNUSED** (remove if empty)
- âŒ `data/eval/model_outputs.csv` - **OLD TEST FILE** (remove)
- âŒ `data/eval/baseline_validation.csv` - **OLD** (replaced by `baseline_outputs.csv`)
- âŒ `data/eval/distilled_validation.csv` - **OLD** (replaced by `distilled_outputs.csv`)

### Optional Directories
- `data/raw/incomplete/` - Can keep for reference, not used in pipeline

## ğŸ“¦ ESSENTIAL MODEL FILES

### Trained Models (LoRA Adapters)
```
models/distilled/
  â””â”€â”€ <model-name>-lora/      # Generated: LoRA adapter after fine-tuning
      â”œâ”€â”€ adapter_config.json       # âœ… REQUIRED: LoRA configuration
      â”œâ”€â”€ adapter_model.safetensors # âœ… REQUIRED: LoRA weights
      â”œâ”€â”€ tokenizer_config.json     # âœ… REQUIRED: Tokenizer config
      â”œâ”€â”€ vocab.json                # âœ… REQUIRED: Vocabulary (~1MB)
      â”œâ”€â”€ merges.txt                # âœ… REQUIRED: BPE merges (~450KB)
      â”œâ”€â”€ special_tokens_map.json   # âœ… REQUIRED: Special tokens
      â”œâ”€â”€ checkpoint-5/             # âŒ OPTIONAL: Training checkpoint (can delete)
      â””â”€â”€ README.md                 # âŒ OPTIONAL: Auto-generated (can delete)
```

**Essential Files for Inference:**
- `adapter_config.json` (~4KB)
- `adapter_model.safetensors` (~4KB)
- `tokenizer_config.json` (~4KB)
- `vocab.json` (~1MB)
- `merges.txt` (~450KB)
- `special_tokens_map.json` (~4KB)

**Can Be Removed (Training Artifacts):**
- `checkpoint-5/` directory (~1.5MB) - Only needed to resume training
  - Contains: optimizer.pt, scheduler.pt, trainer_state.json, rng_state.pth
  - Safe to delete if only doing inference
- `README.md` (~8KB) - Auto-generated by HuggingFace

**Total Size:** ~3MB (can reduce to ~1.5MB by removing checkpoint)

### Empty Directories (Can Create When Needed)
- `models/baseline/` - Empty, can create if storing baseline models
- `models/teacher/` - Empty, can create if storing teacher models
- `models/checkpoints/` - Empty, created during training

## ğŸš€ MINIMUM FILE SET TO RUN PIPELINE

To run the complete pipeline, you need:

### Scripts (9 files)
1. `scripts/run_pipeline.py`
2. `scripts/dataset_generation/generate_examples.py`
3. `scripts/cleaning/filter_bad.py`
4. `scripts/cleaning/repair_smt.py`
5. `scripts/verify/check_validity.py`
6. `scripts/distillation/format_jsonl.py`
7. `scripts/distillation/finetune.py`
8. `scripts/refine/refiner.py`
9. `scripts/eval/evaluate.py`

### Configuration
- `requirements.txt`
- `README.md`

### Input Data (or generate it)
- `data/raw/*.csv` (4 domain CSVs)

## ğŸ”§ CLEANUP COMMANDS

To remove duplicate/unused files:

```bash
# Remove duplicate scripts
rm scripts/dataset_generation/generate.py
rm scripts/cleaning/clean_examples.py
rm scripts/distillation/finetune_lora.py
rm scripts/distillation/format_for_training.py
rm scripts/refine/iterative_refiner.py
rm scripts/eval/evaluate_models.py

# Remove analysis tools (optional)
rm scripts/cleaning/analyze_scheduling.py
rm scripts/cleaning/check_scheduling.py
rm scripts/cleaning/check_scheduling_simple.py

# Remove old data files
rm data/cleaned/cleaned_dataset.csv
rm data/cleaned/jsonl/training_dataset.jsonl
rm data/eval/model_outputs.csv
rm data/eval/baseline_validation.csv
rm data/eval/distilled_validation.csv

# Clean up model training artifacts (optional - only if not resuming training)
rm -rf models/distilled/tiny-gpt2-lora/checkpoint-5/
rm models/distilled/tiny-gpt2-lora/README.md  # Auto-generated, optional
```

## âœ… VERIFICATION CHECKLIST

Before running pipeline, verify:

- [ ] All 9 essential scripts exist
- [ ] `requirements.txt` is up to date
- [ ] Input CSVs exist in `data/raw/` OR generation script is ready
- [ ] Ollama is installed and accessible
- [ ] Python dependencies are installed (`pip install -r requirements.txt`)
- [ ] Z3 is installed (`pip install z3-solver`)
- [ ] HuggingFace transformers/peft are installed
- [ ] Model directory exists (`models/distilled/`)

## ğŸ“‹ EXECUTION ORDER

1. **Generate datasets** â†’ `data/raw/*.csv`
2. **Filter** â†’ `data/cleaned/valid.csv` + `data/raw/broken.csv`
3. **Repair** â†’ `data/cleaned/repaired.csv`
4. **Validate** â†’ `data/eval/validation_results.csv`
5. **Format** â†’ `data/cleaned/training.jsonl`
6. **Train** â†’ `models/distilled/<model>-lora/`
7. **Refine** â†’ `data/eval/refined_outputs.csv`
8. **Evaluate** â†’ `data/eval/final_metrics.json`

All of this can be done with one command:
```bash
python scripts/run_pipeline.py
```

